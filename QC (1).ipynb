{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b535e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/31 11:05:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/31 11:05:54 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = (SparkConf()\n",
    "       .set(\"spark.driver.bindAddress\", \"0.0.0.0\")\\\n",
    "       .set(\"spark.submit.deployMode\", \"client\")\\\n",
    "       .set(\"spark.kubernetes.driver.pod.name\", os.getenv(\"MY_POD_NAME\"))\\\n",
    "       .set(\"spark.kubernetes.authenticate.subdmission.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\\\n",
    "       .set(\"spark.kubernetes.authenticate.submission.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\\\n",
    "       .set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-driver\")\\\n",
    "       .set(\"spark.kubernetes.namespace\", os.getenv(\"MY_POD_NAMESPACE\"))\\\n",
    "       .set(\"spark.executor.instances\", \"1\")\\\n",
    "       .set(\"spark.dynamicAllocation.maxExecutors\", \"2\")\\\n",
    "       .set(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "       .set(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\")\\\n",
    "       .set(\"spark.driver.host\", \"spark-driver\")\\\n",
    "       .set(\"spark.driver.port\", \"20020\")\\\n",
    "       .set(\"spark.kubernetes.executor.request.cores\", \"1\")\\\n",
    "       .set(\"spark.kubernetes.executor.limit.cores\", \"2\")\\\n",
    "       .set(\"spark.executor.memory\", \"12000m\")\\\n",
    "       .set(\"spark.kubernetes.container.image\", \"klovercloud/airflow-spark-k8s-driver:3.1.2\")\\\n",
    "       .set(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\\\n",
    "       .set(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\\\n",
    "       .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\")\\\n",
    "       .set(\"spark.sql.catalog.spark_catalog.type\",\"hive\")\\\n",
    "       .set(\"spark.hadoop.hive.metastore.uris\",\"thrift://hive-metastore.analytics-dev:9083\")\\\n",
    "       .set(\"spark.hadoop.hive.metastore.warehouse.dir\",\"s3a://bigdata-dev-cmfcknil/warehouse/\")\\\n",
    "       .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\",\"true\")\\\n",
    "       .set(\"spark.hadoop.fs.s3a.path.style.access\",\"true\")\\\n",
    "       .set(\"spark.hadoop.fs.s3a.endpoint\",\"https://kcs3.bd-1.wpc.waltonelectronics.com\")\\\n",
    "       .set(\"spark.hadoop.fs.s3a.access.key\",\"30J9IFFY75NVQZVHG4D4\")\\\n",
    "       .set(\"spark.hadoop.fs.s3a.secret.key\",\"WMGk6kqq5erjqZjnqo0QItSQ2zuoNWEpHIphsTgR\")\\\n",
    "       .set(\"spark.hadoop.fs.s3a.attempts.maximum\",\"1\")\\\n",
    "       .set(\"spark.hadoop.fs.s3a.connection.establish.timeout\",\"500\")\\\n",
    "       .set(\"spark.hadoop.datanucleus.autoCreateSchema\",\"true\")\\\n",
    "       .set(\"spark.hadoop.datanucleus.fixedDatastore\",\"false\"));\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"k8s://https://kubernetes.default:443\") \\\n",
    "            .appName(\"spark-test-app\") \\\n",
    "            .config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb31e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/31 11:06:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pqtDF=spark.read.parquet(\"s3a://bigdata-dev-cmfcknil/raw/idl/ProductRegistration/tblProductRegistration/dbo/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f50a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|      ebs|\n",
      "|      pos|\n",
      "|   pqspos|\n",
      "|      reg|\n",
      "|    tmpdb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa444698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------+-----------+\n",
      "|database|tableName                             |isTemporary|\n",
      "+--------+--------------------------------------+-----------+\n",
      "|reg     |dbo___apicurrentcredit                |false      |\n",
      "|reg     |dbo___apifailedstatus                 |false      |\n",
      "|reg     |dbo___apismslog                       |false      |\n",
      "|reg     |dbo___blacklisteddealerphonenumbers   |false      |\n",
      "|reg     |dbo___bumperofferdetails              |false      |\n",
      "|reg     |dbo___bumperoffersmsmaster            |false      |\n",
      "|reg     |dbo___computerproductoffer            |false      |\n",
      "|reg     |dbo___deleted_bumperofferdetails      |false      |\n",
      "|reg     |dbo___deleted_bumperoffersmsmaster    |false      |\n",
      "|reg     |dbo___deleted_tblproductregistration  |false      |\n",
      "|reg     |dbo___deleted_tbltvactivation         |false      |\n",
      "|reg     |dbo___fixedoffervalues                |false      |\n",
      "|reg     |dbo___offervaluetype                  |false      |\n",
      "|reg     |dbo___partycorrectnew                 |false      |\n",
      "|reg     |dbo___partycorre_old                  |false      |\n",
      "|reg     |dbo___pos_sales_return                |false      |\n",
      "|reg     |dbo___tbl_contacts                    |false      |\n",
      "|reg     |dbo___tblcustomerproductreglimit      |false      |\n",
      "|reg     |dbo___tbldealarstock                  |false      |\n",
      "|reg     |dbo___tbldealerphysicalinventorydetail|false      |\n",
      "|reg     |dbo___tbldistributorlocation          |false      |\n",
      "|reg     |dbo___tblinvalidproductregistration   |false      |\n",
      "|reg     |dbo___tbl_plaza_dealer                |false      |\n",
      "|reg     |dbo___tblproductdetaildata            |false      |\n",
      "|reg     |dbo___tblproductregistration          |false      |\n",
      "|reg     |dbo___tblregistrationlocationdetail   |false      |\n",
      "|reg     |dbo___tblsmsmaster                    |false      |\n",
      "|reg     |dbo___tbltvactivation                 |false      |\n",
      "|reg     |dbo___temp_desco_data                 |false      |\n",
      "|reg     |dbo___updatedapifailedstatus          |false      |\n",
      "|reg     |dbo___verifiedidentifications         |false      |\n",
      "|reg     |dbo___verifiedloyalitycardinfo        |false      |\n",
      "|reg     |dbo___wecareredeempointdetails        |false      |\n",
      "|reg     |dbo___wecareredeempointsummary        |false      |\n",
      "+--------+--------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES FROM reg;\").show(200,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5e9dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10624633"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pqtDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b07332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql(\"select * from reg.dbo___tblproductregistration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56605caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10680138"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "939d8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as funcs\n",
    "pqtDF = pqtDF.withColumn(\"date_of_registration\",funcs.to_date(funcs.col(\"ProductRegistrationDate\")))\n",
    "df = df.withColumn(\"date_of_registration\",funcs.to_date(funcs.col(\"ProductRegistrationDate\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9113d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "pqtDF = pqtDF.withColumn('Month',month(pqtDF.date_of_registration))\n",
    "df = df.withColumn('Month',month(df.date_of_registration))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe241422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "pqtDF = pqtDF.withColumn('Year',year(pqtDF.date_of_registration))\n",
    "df = df.withColumn('Year',year(df.date_of_registration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b8933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/30 10:46:50 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "yearwiserowcount = pqtDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "085b8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearwiserowcount.createOrReplaceTempView(\"yearcount\")\n",
    "df.createOrReplaceTempView(\"countyear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f7b6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|count(1)|\n",
      "+----+--------+\n",
      "|2017|  518623|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year2017count = spark.sql(\"SELECT Year,count(*) FROM yearcount where Year=2017 group by Year\")\n",
    "year2017count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78cd3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2017count = year2017count.withColumnRenamed(\"count(1)\",\"totalrowcount2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceaa7dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|count(1)|\n",
      "+----+--------+\n",
      "|2017|  518623|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2017count = spark.sql(\"SELECT Year,count(*) FROM countyear where Year=2017 group by Year\")\n",
    "df2017count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "458b0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017count = df2017count.withColumnRenamed(\"count(1)\",\"icebergrowcount2017\")\n",
    "df2017count = df2017count.withColumnRenamed(\"Year\",\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd5f6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck17 = year2017count.join(df2017count,year2017count.totalrowcount2017 == df2017count.icebergrowcount2017,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b72a21ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+------+-------------------+\n",
      "|Year|totalrowcount2017|bochor|icebergrowcount2017|\n",
      "+----+-----------------+------+-------------------+\n",
      "|2017|           518623|  2017|             518623|\n",
      "+----+-----------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowcheck17.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55c9caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck17 = rowcheck17.drop(\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10d5b4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------------+\n",
      "|Year|totalrowcount2017|icebergrowcount2017|\n",
      "+----+-----------------+-------------------+\n",
      "|2017|           518623|             518623|\n",
      "+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowcheck17.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e97c390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2018count = spark.sql(\"SELECT Year,count(*) FROM yearcount where Year=2018 group by Year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70475818",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2018count = year2018count.withColumnRenamed(\"count(1)\",\"totalrowcount2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "272753a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018count = spark.sql(\"SELECT Year,count(*) FROM countyear where Year=2018 group by Year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a48d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018count = df2018count.withColumnRenamed(\"count(1)\",\"icebergrowcount2018\")\n",
    "df2018count = df2018count.withColumnRenamed(\"Year\",\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7461712",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck18 = year2018count.join(df2018count,year2018count.totalrowcount2018 == df2018count.icebergrowcount2018,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4ed44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck18 = rowcheck18.drop(\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfaf1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------------+\n",
      "|Year|totalrowcount2018|icebergrowcount2018|\n",
      "+----+-----------------+-------------------+\n",
      "|2018|          1955039|            1955039|\n",
      "+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowcheck18.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f819329",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2019count = spark.sql(\"SELECT Year,count(*) FROM yearcount where Year=2019 group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7856dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2019count = year2019count.withColumnRenamed(\"count(1)\",\"totalrowcount2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d5c1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2019count = spark.sql(\"SELECT Year,count(*) FROM countyear where Year=2019 group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d7f5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2019count = df2019count.withColumnRenamed(\"count(1)\",\"icebergrowcount2019\")\n",
    "df2019count = df2019count.withColumnRenamed(\"Year\",\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ed98977",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck19 = year2019count.join(df2019count,year2019count.totalrowcount2019 == df2019count.icebergrowcount2019,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f002569",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck19 = rowcheck19.drop(\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bdc3ed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------------+\n",
      "|Year|totalrowcount2019|icebergrowcount2019|\n",
      "+----+-----------------+-------------------+\n",
      "|2019|          2716880|            2716880|\n",
      "+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowcheck19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0024c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2020count = spark.sql(\"SELECT Year,count(*) FROM yearcount where Year=2020 group by Year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0d2d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2020count = year2020count.withColumnRenamed(\"count(1)\",\"totalrowcount2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69562216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2020count = spark.sql(\"SELECT Year,count(*) FROM countyear where Year=2020 group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e357e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2020count = df2020count.withColumnRenamed(\"count(1)\",\"icebergrowcount2020\")\n",
    "df2020count = df2020count.withColumnRenamed(\"Year\",\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a50c6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck20 = year2020count.join(df2020count,year2020count.totalrowcount2020 == df2020count.icebergrowcount2020,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5be9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck20 = rowcheck20.drop(\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7daa4158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------------+\n",
      "|Year|totalrowcount2020|icebergrowcount2020|\n",
      "+----+-----------------+-------------------+\n",
      "|2020|          2275356|            2275356|\n",
      "+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowcheck20.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d91c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2021count = spark.sql(\"SELECT Year,count(*) FROM yearcount where Year=2021 group by Year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95d91c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2021count = year2021count.withColumnRenamed(\"count(1)\",\"totalrowcount2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3dc6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2021count = spark.sql(\"SELECT Year,count(*) FROM countyear where Year=2021 group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be44e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2021count = df2021count.withColumnRenamed(\"count(1)\",\"icebergrowcount2021\")\n",
    "df2021count = df2021count.withColumnRenamed(\"Year\",\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e4d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4af87bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck21 = year2021count.join(df2021count,year2021count.totalrowcount2021 == df2021count.icebergrowcount2021,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "477efe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcheck21 = rowcheck21.drop(\"bochor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61e0fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = rowcheck17.union(rowcheck18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a038e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = row.union(rowcheck19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6355ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row2 = row1.union(rowcheck20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa9ab34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "row3 = row2.union(rowcheck21,['Year'],how='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f86efb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+\n",
      "|Year|totalrowcount2017|icebergrowcount2017|totalrowcount2018|icebergrowcount2018|totalrowcount2019|icebergrowcount2019|totalrowcount2020|icebergrowcount2020|totalrowcount2021|icebergrowcount2021|\n",
      "+----+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+\n",
      "|2018|             null|               null|          1955039|            1955039|             null|               null|             null|               null|             null|               null|\n",
      "|2019|             null|               null|             null|               null|          2716880|            2716880|             null|               null|             null|               null|\n",
      "|2020|             null|               null|             null|               null|             null|               null|          2275356|            2275356|             null|               null|\n",
      "|2017|           518623|             518623|             null|               null|             null|               null|             null|               null|             null|               null|\n",
      "|2021|             null|               null|             null|               null|             null|               null|             null|               null|          3055323|            3055323|\n",
      "+----+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49e6155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row3 = row3.dropDuplicates([\"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 334:(18 + 2) / 144][Stage 335:> (0 + 0) / 14][Stage 338:>(0 + 0) / 144]]]\r"
     ]
    }
   ],
   "source": [
    "row3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9429ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2021 = pqtDF.filter(pqtDF.Year == 2021)\n",
    "df2021 = df.filter(df.Year == 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a7b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pqtDFjune2021 = pqtDF2021.filter(pqtDF2021.Month == 6)\n",
    "dfjune2021 = df2021.filter(df.Month == 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3486e416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "323974"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pqtDFjune2021.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf94859",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjune2021.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6e5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDFfirst6months = pqtDF.filter((pqtDF.Month == 1) | (pqtDF.Month == 2) | (pqtDF.Month == 3) | (pqtDF.Month == 4) | (pqtDF.Month == 5) | (pqtDF.Month == 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ec03c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4619775"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pqtDFfirst6months.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76800dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "pqtDF = pqtDF.withColumn('Year',year(pqtDF.date_of_registration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb9f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2017 = pqtDF.filter(pqtDF.Year == 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af3ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2017.createOrReplaceTempView(\"2017_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a76af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq17 = spark.sql(\"SELECT Year,count(*) from 2017_table group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8231efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|count(1)|\n",
      "+----+--------+\n",
      "|2017|  518623|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pq17.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf660c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq17 = pq17.withColumnRenamed(\"count(1)\",\"totalrows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "057e78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2018 = pqtDF.filter(pqtDF.Year == 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dbca4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2018.createOrReplaceTempView(\"2018_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e84aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq18 = spark.sql(\"SELECT Year,count(*) from 2018_table group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d7a0cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|count(1)|\n",
      "+----+--------+\n",
      "|2018| 1955039|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pq18.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5916e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq18 = pq18.withColumnRenamed(\"count(1)\",\"totalrows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "959724ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2019 = pqtDF.filter(pqtDF.Year == 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b52e673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2019.createOrReplaceTempView(\"2019_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4eb6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq19 = spark.sql(\"SELECT Year,count(*) from 2019_table group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79218bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|count(1)|\n",
      "+----+--------+\n",
      "|2019| 2716880|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pq19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d32e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq19 = pq19.withColumnRenamed(\"count(1)\",\"totalrows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92cce163",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2020 = pqtDF.filter(pqtDF.Year == 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f058c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2020.createOrReplaceTempView(\"2020_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdcc2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq20 = spark.sql(\"SELECT Year,count(*) from 2020_table group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7b73f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|count(1)|\n",
      "+----+--------+\n",
      "|2020| 2275356|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pq20.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2df895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq20 = pq20.withColumnRenamed(\"count(1)\",\"totalrows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7606c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2021 = pqtDF.filter(pqtDF.Year == 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09c1eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqtDF2021.createOrReplaceTempView(\"2021_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d706ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq21 = spark.sql(\"SELECT Year,count(*) from 2021_table group by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35737d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|count(1)|\n",
      "+----+--------+\n",
      "|2021| 3055323|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pq21.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b33c2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq21 = pq21.withColumnRenamed(\"count(1)\",\"totalrows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb26d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pq17.union(pq18)\n",
    "result1 = result.union(pq19)\n",
    "result2 = result1.union(pq20)\n",
    "result3 = result2.union(pq21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02885cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/29 08:51:29 WARN WatchConnectionManager: Exec Failure        (1 + 1) / 14]\n",
      "java.io.EOFException\n",
      "\tat okio.RealBufferedSource.require(RealBufferedSource.java:61)\n",
      "\tat okio.RealBufferedSource.readByte(RealBufferedSource.java:74)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.readHeader(WebSocketReader.java:117)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:101)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)\n",
      "\tat okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)\n",
      "\tat okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)\n",
      "\tat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 146:==================================================>  (358 + 2) / 375]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|totalrows|\n",
      "+----+---------+\n",
      "|2017|   518623|\n",
      "|2018|  1955039|\n",
      "|2019|  2716880|\n",
      "|2020|  2275356|\n",
      "|2021|  3055323|\n",
      "+----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/29 09:01:30 WARN WatchConnectionManager: Exec Failure                     \n",
      "java.io.EOFException\n",
      "\tat okio.RealBufferedSource.require(RealBufferedSource.java:61)\n",
      "\tat okio.RealBufferedSource.readByte(RealBufferedSource.java:74)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.readHeader(WebSocketReader.java:117)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:101)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)\n",
      "\tat okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)\n",
      "\tat okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)\n",
      "\tat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/01/29 09:01:31 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: too old resource version: 41511249 (41513669)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onMessage(WatchConnectionManager.java:258)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.onReadMessage(RealWebSocket.java:323)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.readMessageFrame(WebSocketReader.java:219)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:105)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)\n",
      "\tat okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)\n",
      "\tat okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)\n",
      "\tat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "result3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292e7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
